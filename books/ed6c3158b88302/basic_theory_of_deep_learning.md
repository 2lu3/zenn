---
title: "深層学習の基礎理論"
---


数学的な厳密さ、用語の厳密さよりイメージのしやすさを優先して説明します。

# はじめに


この記事には間違いが含まれています。


# ニューロン

小学生のAくんを例にして説明しましょう。

小学生のAくんは、明日学校でテストがあるかどうかを忘れてしまいました。明日テストがあるなら、これからテスト勉強をしないといけませんが、テストがないなら勉強をしたくありません。

頑張って思い出したところ、下の3つの手がかりがありました。

- 友達は、テストの前日に「勉強していないどうしよー」とAくんに言ってきますが、塾のテストと学校のテストを勘違いしていることがあります
- 先生は「明日はテストだから家に帰ったら勉強しましょうね」と言うのですが、おっちょこちょいな先生なので間違えているときもあります
- Aくんの極秘データによると、先生は水曜日にテストをすることが多いです

そして、今の状況は下のようになっています。

- 友達は今日、「勉強していないどうしよー」と言ってきました
- 先生は「明日テストです」とは言いませんでした
- 明日は木曜日です

さて、Aくんはテスト勉強をするべきなのか、考えてくれるAIを作ってみましょう。

AIの仕組みは、下の図のようになっています。

![](/images/01/2022-02-11-15-20-58.png)

ニューロンは、左側から数字を受け取り、中で計算をして右側に数字を主力します。具体的な数式は、入力を$x_1, x_2, \cdots ,x_n$、出力を$y$、重み(あとで説明します)を$w_0, w_1, w_2, \cdots , w_n$とすると、

$$
y = w_o + \sum_{i=1}^n{x_i w_i}
$$

となります。数式で書くと少し難しいので、一部に具体的な数値を入れて書いてみると、下の図のようになります。

![](/images/01/2022-02-11-15-37-57.png)

この図での、yを求める式は

$$
y = -0.5 + 0.4x_1 + 0.6x_2 + 0.3x_3
$$

となります。上の状況では、$x_1=1,x_2=0,x_3=0$なので、$y=-0.1$となります。そして、この値が0[^1]より小さいので、このAIは「明日はテストがない」とAくんに伝えます。

[^1]: 0というしきい値は、わかりやすさのために適当に設定したもので、すべての深層学習に対して成り立つものではありません。

意味を簡単に説明すると、Aくんの中で、「友達」、「先生」、「曜日」の信用度は4:6:3になっています。つまり、友達の言うことはそこそこ信用できる、先生はおっちょこちょいだけどさらに信用できる、曜日は間違っていることが多いから少ししか信用できない、ということです。

ちなみに、$w_0$の重みは-0.5となっているので、「友達」、「先生」、「曜日」全てが明日テストがないことを示唆しているなら、-0.5が予想となります。$w_0$がある理由は、$y=ax$だとすべての組み合わせを表現できない(x=0のときy=1など)ので、$y=ax+b$のように定数項を入れる必要があるからです。

この信用度がそれぞれの重みになっています。そして、誤解を恐れずにいえば、深層学習では「すべてのケースに対して正しく予想できる重みを見つける」ことが目的なわけです。

# 層

上の例では、入力層にニューロンが3つ、出力層にニューロンが1つありました。入力層とは、データを受け取る層のことで、「予想」をするときに使うデータの種類と同じだけのニューロンがあります。出力層とは、予測結果を表現する層で、今回はニューロンが1つだけでしたが、複数のニューロンがある出力層もあります。

今回は登場しませんでしたが中間層という層もあり、高度なAIでは大半を中間層が占めています。中間層の数や中間層のニューロンの数によってAIの性能は大きく変化します。これは、中間層が存在することによってより複雑なことを内部で表現できることが理由です。

また、入力層と出力層ではニューロン1つ1つに意味が有りましたが、中間層ではどのニューロンにどのような意味があるのかは原則人間にはわかりません。これを、「ブラックボックス」と言い、深層学習がどうしてその判断に至ったのか人間にはわからない点が問題視されています。


ちなみに、上で説明した数式のみだと、中間層が複数あるモデルで表現できることと、中間層なしの入力層と出力層のみのモデルで表現できることは同じです。言い換えると、いくら中間層を挟んでも結局は入力層のデータに対する線型結合で出力層は表現できます。

次は、この線型結合として表現できてしまう問題を解決するための仕組みを説明します。

# 活性化関数

活性化関数は非線形な関数[^2]なので、非常に複雑な表現ができるようになります。
[^2]: 非線形でない活性化関数「恒等関数」もありますが、活性化関数はほぼすべてが非線形です

活性化関数を$\sigma$をすると、ニューロンの入力と出力の関係は

$$
y = \sigma(w_0 + \sum{w_ix_i})
$$

のようになります。

活性化関数の説明は、[【初心者】ネコでも分かる「活性化関数」ってなに？](https://zenn.dev/nekoallergy/articles/4e224b57a97af9)がわかりやすいです。

活性化関数には様々な種類があります。有名な活性化関数は[活性化関数一覧 (2020)](https://qiita.com/kuroitu/items/73cd401afd463a78115a)で紹介されています。ここに載っていない活性化関数を自分で自作することもできます。

どの活性化関数を選択するかによって、モデルの性能が大きく変わることもあり、いくつか定番の活性化関数があります。

例えば、

- 入力層で使うことが多い関数
  - relu系
  - tanh系
- 中間層で使うことが多い関数
  - 上と同じ
- 出力層で確率を出力したい場合
  - softmax

などです。

# 誤差伝播法

本書で取り扱う深層学習は、入力データと出力データ(正解データ)がセットになっています。そして、優秀なモデルは、入力データから出力データを正確に予想できます。言い換えると、多くのケースでモデルの出力値と正解の値が一致もしくは近い値になっています。逆に言えば、深層学習では出力値が正解の値と一致するように学習します。

簡単のため、出力層にはニューロン1つのみがあり、出力値が1つのみしかない状態を考えます。このとき、正解の値と出力値の差を$L$と置きます。ここで、ある重み$w$に対して$w$を増やすと$L$が大きくなるのか、小さくなるのかは下の式からわかります。

$$
\frac{\partial L}{\partial w}
$$

つまり、$\frac{\partial L}{\partial w}$が正なら$w$を増やすと$L$は大きくなり、負なら$w$を増やすと$L$は小さくなります。
よって、$L$をすべての重み$w$でそれぞれ偏微分すれば、どの重みを増やし、どの重みを減らすべきかがわかります。

ちなみに、ここでいう偏微分は、$L(w+\Delta w)-L(w)$を計算します。

ですが、高性能なモデルであれば、数兆個[^3]というパラメーターがあり、できるだけ計算する回数を減らしたいです。そこで、考えられた方法が誤差伝播法です。これを使えば、計算回数を大幅に減らすことができます。

[^3]: [Googleは兆パラメータのAI言語モデルSwitch Transformerをオープンソース化](https://www.infoq.com/jp/news/2021/04/google-trillion-parameter-ai/)

誤差伝播法を使わない場合、例えば、赤いパラメーターで偏微分を計算する場合、計算する必要のある部分は青色のようになっています。 そして、同じくオレンジ色のパラメーターで偏微分を計算する場合も同じく青色の部分を計算する必要があります。

![](/images/basic_theory_of_deep_learning/2022-02-11-22-41-12.png)

つまり、青色の部分の計算をなんども行う必要があり、できれば1つのパラメーターの計算は一度だけですませたいわけです。

誤差伝播法を使うと、すべての重みの偏微分を1度ずつ計算するだけで十分になります。その図が下のようになっています。

![](/images/basic_theory_of_deep_learning/2022-02-11-22-53-43.png)

まず、赤色の線で囲った部分である、出力層の重みで偏微分します。つまり、$\frac{\partial L}{\partial w_1},\frac{\partial L}{\partial w_2},\frac{\partial L}{\partial w_3}$を求めています。

次に、青色の線で囲った部分の重みで偏微分します。合成関数の微分公式[^4]を用いると、すでに計算した偏微分の値である$\frac{\partial L}{\partial w_1}$と、$\frac{\partial w_1}{\partial w_4}$の掛け算になっているので、新たに計算する部分は$\frac{\partial w_1}{\partial w_4}$だけで十分です。

[^4]: $\frac{dy}{dx}=\frac{dy}{dt}\frac{dt}{dx}$

オレンジ色の線で囲った部分についても同様に、青色のエリアで計算した偏微分の値さえ記憶しておけば、新しく計算する偏微分の回数を最小にすることができます。

このように、出力層の誤差が1つずつ入力層へとさかのぼっていくという特徴から、逆伝播法と名付けられています。

# 最適化関数

逆伝播法によって正解の値と出力値の差が最も小さくなるようなパラメーターを計算できるようになりました。しかし、単純に差が小さくなる方向にパラメーターを更新する(この更新方法は「最急降下法」という名前の最適化関数です)だけでは極小値に陥ってしまう可能性があります。

そのため、よく使われているAdamという最適化関数では、極小値に陥らず、最小値を見つけることができるようなパラメーターの更新を行います。とはいっても、毎回最小値を見つけることができるわけではなく、最小値に近い極小値を見つけていることも多々有ります。

最適化関数については、[【決定版】スーパーわかりやすい最適化アルゴリズム -損失関数からAdamとニュートン法-](https://qiita.com/omiita/items/1735c1d048fe5f611f80)のサイトが非常に詳しく、またわかりやすいため、こちらを読むことをおすすめします。

# 参考文献

[深層学習入門 〜基礎編〜](https://qiita.com/kuroitu/items/221e8c477ffdd0774b6b)
[機械学習という人工知能の技術](https://zenn.dev/ryoatsuta/articles/aeae4990583ff6)
[活性化関数一覧 (2020)](https://qiita.com/kuroitu/items/73cd401afd463a78115a)
[【初心者】ネコでも分かる「活性化関数」ってなに？](https://zenn.dev/nekoallergy/articles/4e224b57a97af9)
[【決定版】スーパーわかりやすい最適化アルゴリズム -損失関数からAdamとニュートン法-](https://qiita.com/omiita/items/1735c1d048fe5f611f80)
[【機械学習】誤差逆伝播法のコンパクトな説明](https://qiita.com/sand/items/2d783a12c575fb949c6e)
[【人工知能】誤差逆伝播法とは。ニューラルネットワークの学習の仕組み。](https://fresopiya.com/2019/05/07/backpropagation/)
